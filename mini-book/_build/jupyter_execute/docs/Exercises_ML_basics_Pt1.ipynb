{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fe0ce8-b7ef-40d0-8d03-f85630d3ae8d",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1f53d-6a3e-4c47-afd6-f7af64c1bd20",
   "metadata": {},
   "source": [
    "These exercises cover basic ML strategies, including regression, interpolation, hyperparameter tuning, and dimensionality reduction. <br><br>\n",
    "**NOTE:** These exercises cover only a small sample of ML techniques. All ML will be implemented in the *scikit-learn* Python package here. Documentation can be found at https://scikit-learn.org/stable/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c1165-e92b-49ee-a8b1-acfed1ad01d1",
   "metadata": {},
   "source": [
    "(ml-ex-1)=\n",
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33441ff2",
   "metadata": {},
   "source": [
    "Attached to this exercise is an IR spectrum for ethanol. Use the *pandas* package to import the Excel (CSV) file as a Python dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d2476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0c4ba",
   "metadata": {},
   "source": [
    "Fit the data using a radial basis function kernel regression. This is a **non-parametric** model that performs intepolation. Set the Gaussian width (sigma) to 100 and train the model on all data points. Report the mean absolute error (MAE) of the model and plot the data with the fitted model on the same plot. Be sure to color data points used for model training differently than those that were only part of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ecc3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e984abe",
   "metadata": {},
   "source": [
    "Repeat the above task but only train the model on every third data point. What happens to the fit and to the MAE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1b34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1922a",
   "metadata": {},
   "source": [
    "Lastly, repeat the fitting with a Gaussian width (sigma) of 1. Explain what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fbb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d656717",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54072710",
   "metadata": {},
   "source": [
    "This exercise is a continuation of the previous and uses the same dataset. <br><br>\n",
    "First, use kernel ridge regression (KRR) to fit the IR data. KRR is simply kernel regression from Exercise 1 with an added L2 regularization term to control smoothness. How does this model compare to your model in Exercise 1? Test your model with a regularization strength (alpha) of 0.1 and 10. What happens as alpha goes to infinity? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c25f1",
   "metadata": {},
   "source": [
    "Next, try using L1 regularization (LASSO) in place of L2. Note that, unlike L2, L1 regularization will remove unnecessary features entirely. For this exercise, use *sklearn*'s GridSearchCV tool with 5-fold cross validation to automatically perform hyperparameter tuning. Test regularization strengths in the range alpha = [0.0001, 0.001, 0.01, 0.1, 1.0, 10]. Comment on what happends as alpha increases. Plot the best model with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f88504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623f090",
   "metadata": {},
   "source": [
    "(ml-ex-3)=\n",
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8424ed9",
   "metadata": {},
   "source": [
    "This exercise will explore Principal Component Analysis as an unsupervised tool for dimensionality reduction. Begin by importing the diabetes toy dataset from the *sklearn* package. This dataset predicts a patient's susceptibility to diabetes based on ten input features. Once imported, use feature scaling to normalize the features (stanadrd scaler).<br><br> https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbbe3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd8616",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) works by orthogonalizing all features and by identifying features with the greatest amount of variance. Apply PCA to the diabetes feature set. Keep all 10 principal component features and plot their covariance matrix. What do you notice and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96ddddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d1c5a",
   "metadata": {},
   "source": [
    "Use PCA to identify the minimum number of features needed to capture 90% of the data's variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f07b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d515404",
   "metadata": {},
   "source": [
    "Utilize the function below and modify the code to create a loadings plot with the first two principal components. Identify which features from the original dataset may be unnecessary. This can be done because features with the same weights in principal component analysis behave identically and are therefore redundant. <br>*HINT:* Use the pca.components_ routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58304a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom - made plotting to plot loadings \n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    #plt.scatter(xs * scalex,ys * scaley, c = Y1)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee978a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function. Use only the 2 PCs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}